# ğŸ—ï¸ Data Lakehouse with Modern Technologies  

![Architecture](https://github.com/user-attachments/assets/ef98d59f-56f9-41ad-a450-5818f9237a55)


## ğŸ“Œ Overview  
This project demonstrates how to build a **Data Lakehouse** using modern open-source technologies.  
It integrates **Kafka**, **Spark Streaming**, **Apache Iceberg**, **Apache Nessie**, **Trino**, and **DBT** to create an efficient data pipeline.  

ğŸ”¹ **Streaming Layer:** Spark Streaming / Flink processes real-time Kafka events.  
ğŸ”¹ **Batch Layer:** Apache Spark and AirByte handle batch ingestion into an **S3 Minio** data lake.  
ğŸ”¹ **Metadata Layer:** Apache Nessie is used as the Iceberg catalog, backed by PostgreSQL.  
ğŸ”¹ **Transformation Layer:** Trino and DBT transform and store data in **Vertica** for analytics.  
ğŸ”¹ **Orchestration:** Apache Airflow schedules and manages all ETL/ELT workflows.  
ğŸ”¹ **Synthetic Data:** Generated with `sdv` Python library from **SalesDB_v1** dataset.  
ğŸ”¹ **Visualization:** Dashboards built in **Tableau, Superset, or Power BI**.  

## ğŸš€ Tech Stack  
| Category          | Technology |
|------------------|------------|
| **Streaming**    | Kafka, Spark Streaming, Flink |
| **Storage**      | S3 Minio (Apache Iceberg) |
| **Metadata**     | Apache Nessie, PostgreSQL |
| **ETL/ELT**      | Airflow, DBT, Apache Spark |
| **Query Engine** | Trino, Vertica |
| **Orchestration**| Apache Airflow |
| **Dashboarding** | Tableau, Superset, Power BI |

---

## ğŸ¯ Architecture  

### 1ï¸âƒ£ **Data Ingestion**  
- Streaming events flow from **Kafka** into **Spark Streaming / Flink**.  
- Batch data is ingested via **AirByte** into **S3 Minio (Iceberg format)**.  

### 2ï¸âƒ£ **Storage & Metadata Management**  
- Raw (`raw`) and Operational (`ods`) data are stored in **S3 Minio (Apache Iceberg)**.  
- Apache Nessie acts as the metadata catalog, tracking schema versions and changes.  

### 3ï¸âƒ£ **Transformation & Querying**  
- **DBT + Trino** handle transformations.  
- Processed marts (`marts`) are stored in **Vertica** for BI consumption.  

### 4ï¸âƒ£ **Orchestration & Visualization**  
- **Apache Airflow** schedules all ETL/ELT jobs.  
- Dashboards are created using **Tableau, Superset, or Power BI**.  

---

## ğŸ“¸ Screenshots  
ğŸ”¹ **Kafka UI** (Monitor real-time events)  
![kafka](https://github.com/user-attachments/assets/2a6224d8-c22a-4a64-8196-7c9ce80ebc0d)

ğŸ”¹ **S3 Minio Browser** (View stored Iceberg tables)  
ğŸ”¹ **Apache Nessie UI** (Track schema changes)  
ğŸ”¹ **Airflow DAGs** (Monitor ETL workflows)  
ğŸ”¹ **DBT Lineage Graph** (View transformation dependencies)  
ğŸ”¹ **BI Dashboards** (Analytics & insights)  

---

## ğŸ—ï¸ **Setup & Deployment**  
This project is fully containerized using **Docker Compose**.  

### ğŸ”§ **Prerequisites**  
- Docker & Docker Compose  
- Python (for data generation)  

### ğŸ“¥ **Clone Repository**  
```bash
git clone https://github.com/Turakulov/datalakehouse.git
cd datalakehouse
```

### â–¶ï¸ **Start the Environment**
```bash
docker-compose build

docker-compose up -d
```

### ğŸ” **Verify Services**  
- **Minio Console:** [http://localhost:9001](http://localhost:9001)  
- **Trino UI:** [http://localhost:8083](http://localhost:8083)  
- **Kafka UI:** [http://localhost:9999](http://localhost:9999)  
- **Airflow UI:** [http://localhost:8090](http://localhost:8090)  
- **Vertica (SQL Access):** `jdbc:vertica://localhost:5433/db`  

---

## ğŸ› ï¸ **Project Structure**  
```bash
ğŸ“‚ datalakehouse
â”œâ”€â”€ ğŸ“‚ airflow/        # Apache Airflow DAGs
â”œâ”€â”€ ğŸ“‚ spark/          # Spark Streaming jobs
â”œâ”€â”€ ğŸ“‚ kafka/          # Kafka configurations
â”œâ”€â”€ ğŸ“‚ trino/          # Trino catalogs
â”œâ”€â”€ ğŸ“‚ minio/          # Minio storage setup
â”œâ”€â”€ ğŸ“‚ nessie/         # Apache Nessie metadata
â”œâ”€â”€ ğŸ“‚ postgres/       # Postgres storage setup for Apache Iceberg metadata
â”œâ”€â”€ ğŸ“‚ vertica/        # Vertica storage setup for datamarts and OLAP queries
â”œâ”€â”€ ğŸ“œ docker-compose.yaml  # Docker environment
â””â”€â”€ ğŸ“œ README.md       # Project documentation
```

## ğŸ“Œ **Contributing**
ğŸ”¹ Fork the repo & create a feature branch.  
ğŸ”¹ Submit a pull request with detailed changes.  


